{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mquYPMlsb28q"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# Hier-EgoPack: Hierarchical Egocentric Video Understanding with Diverse Task Perspectives\n",
        "\n",
        "[Simone Alberto Peirone](https://scholar.google.com/citations?user=K0efPssAAAAJ), [Francesca Pistilli](https://scholar.google.com/citations?user=7MJdvzYAAAAJ), [Antonio Alliegro](https://scholar.google.com/citations?user=yQqW5q0AAAAJ), [Tatiana Tommasi](https://scholar.google.com/citations?user=ykFtI-QAAAAJ), [Giuseppe Averta](https://scholar.google.com/citations?user=i4rm0tYAAAAJ)\n",
        "\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "<a href='https://arxiv.org/abs/2502.02487' style=\"margin: 10px\"><img src='https://img.shields.io/badge/Paper-Arxiv:2502.02487-red'></a>\n",
        "<a href='https://sapeirone.github.io/hier-egopack/' style=\"margin: 10px\"><img src='https://img.shields.io/badge/Project-Page-Green'></a>\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sapeirone/hier-egopack/blob/main/quickstart.ipynb\" style=\"margin: 10px\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "✨ <strong>This paper extends our previous work <a href=\"https://sapeirone.github.io/EgoPack/\">A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives\" (CVPR 2024)</a></strong> ✨\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<strong>Abstract:</strong>\n",
        "\n",
        "Our comprehension of video streams depicting human activities is naturally multifaceted: in just a few moments, we can grasp what is happening, identify the relevance and interactions of objects in the scene, and forecast what will happen soon, everything all at once. To endow autonomous systems with such a holistic perception, learning how to correlate concepts, abstract knowledge across diverse tasks, and leverage tasks synergies when learning novel skills is essential.\n",
        "A significant step in this direction is EgoPack, a unified framework for understanding human activities across diverse tasks with minimal overhead. EgoPack promotes information sharing and collaboration among downstream tasks, essential for efficiently learning new skills.\n",
        "In this paper, we introduce Hier-Egopack, which advances EgoPack by enabling reasoning also across diverse temporal granularities, which expands its applicability to a broader range of downstream tasks.\n",
        "To achieve this, we propose a novel hierarchical architecture for temporal reasoning equipped with a GNN layer specifically designed to tackle the challenges of multi-granularity reasoning effectively.\n",
        "We evaluate our approach on multiple Ego4d benchmarks involving both clip-level and frame-level reasoning, demonstrating how our hierarchical unified architecture effectively solves these diverse tasks simultaneously.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAqFCyZ0YL1E"
      },
      "source": [
        "This notebook allows to quickly setup a Google Colab environment for running Hier-EgoPack on the Moment Queries task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6mpKHQ0u7WN"
      },
      "outputs": [],
      "source": [
        "!git clone git@github.com:sapeirone/hier-egopack.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-s2XDhceRj"
      },
      "source": [
        "## Step 0: Dataset Access\n",
        "To access the Ego4d dataset you need to fill the registration form here. Then, you will receive by email an access key with its associated secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm0wiRgtcuV2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = \"\"\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu70rxjyeyxl"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Set up the AWS CLI\n",
        "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
        "unzip -o awscliv2.zip >/dev/null\n",
        "sudo ./aws/install >/dev/null 2>&1\n",
        "aws configure set aws_access_key_id \"$AWS_ACCESS_KEY_ID\" && aws configure set aws_secret_access_key \"$AWS_SECRET_ACCESS_KEY\"\n",
        "rm \"awscliv2.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI2lC-4NeasE"
      },
      "outputs": [],
      "source": [
        "!pip install ego4d\n",
        "!mkdir -p /content/hier-egopack/data/ego4d/raw/annotations/v1\n",
        "!ego4d --output_directory=/content/hier-egopack/data/ego4d/raw --datasets annotations --benchmarks fho moments -y --version v1\n",
        "# adjust the directory structure to match the one required by hier-egopack\n",
        "!mv /content/hier-egopack/data/ego4d/raw/v1/annotations/* /content/hier-egopack/data/ego4d/raw/annotations/v1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw2PLE-UuZYO"
      },
      "source": [
        "### Download EgoVLP pre-extracted features\n",
        "\n",
        "Download the pre-extracted features using EgoVLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0fnbJ-kslC0"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/hier-egopack/data/ego4d/raw/features/egovlp/\n",
        "!wget http://sapeirone.it/data/hier-egopack/egovlp_trainval_features.zip\n",
        "\n",
        "!unzip -j /content/egovlp_trainval_features.zip -d /content/hier-egopack/data/ego4d/raw/features/egovlp/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3piTlREcv2n"
      },
      "source": [
        "## Step 2: Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_IVBJeora5So"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip3 install torch_geometric\n",
        "!pip install ego4d\n",
        "!pip3 install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
        "!cd /content/hier-egopack && pip3 install -r requirements.txt\n",
        "\n",
        "# install nms\n",
        "!cd /content/hier-egopack/libs/utils/ && python setup.py install --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zp2rxsrXtFC"
      },
      "source": [
        "## Step 3: Single task training (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0GAAt1CbAtF"
      },
      "outputs": [],
      "source": [
        "!cd /content/hier-egopack/ && python train_single_task.py --config-name=mq features=ego4d/egovlp eval_interval=-1 lr_warmup=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugmhBGNFX2aG"
      },
      "source": [
        "## Step 4: EgoPack task training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_beX32iyjIi"
      },
      "outputs": [],
      "source": [
        "# Download the mtl checkpoint\n",
        "!wget https://sapeirone.it/data/hier-egopack/ego4d_ar-ego4d_lta-ego4d_oscc-ego4d_pnr_2024-11-09-13-36.pth\n",
        "!mv ego4d_ar-ego4d_lta-ego4d_oscc-ego4d_pnr_2024-11-09-13-36.pth ckpt.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns73ib67FS3u"
      },
      "outputs": [],
      "source": [
        "!cd /content/hier-egopack/ && python train_egopack.py --config-name=mq resume_from=/content/ckpt.pth lr_warmup=True egopack.depth=1 egopack.hidden_size=256 egopack.conv_depth=1 egopack.k=4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMBWBq9SqJfqpNUX4ArsmSB",
      "gpuType": "A100",
      "mount_file_id": "https://github.com/sapeirone/ego-graph/blob/tpami-release/quickstart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
